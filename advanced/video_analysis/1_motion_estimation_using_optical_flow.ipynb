{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ4fwhDMEtjc"
      },
      "source": [
        "# Motion Estimation using Optical Flow\n",
        "Recent advancements in computer vision have enabled machines to interpret their surroundings using techniques like object detection, which identifies instances of specific object classes, and semantic segmentation, which classifies each pixel in an image.\n",
        "\n",
        "Despite these advancements, most real-time video processing implementations focus solely on analyzing spatial relationships within individual frames (x, y), ignoring temporal information (t). In other words, they treat each frame as an independent image and analyze them separately without considering connections between consecutive frames. But what if we need to understand the relationships between frames? For instance, tracking vehicle motion across frames to calculate its velocity and predict its future position requires incorporating temporal context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qETZybeKEtjo"
      },
      "source": [
        "Or, alternatively, what if we require information on human pose relationships between consecutive frames to recognize human actions such as archery, baseball, and basketball?\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "In this experiment, we will learn what Optical Flow is, how to implement its two main variants (sparse and dense), and also get a big picture of more recent approaches involving deep learning and promising future directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlG8M_rVEtjr"
      },
      "source": [
        "## 1. What is Optical Flow?\n",
        "Let us begin with a high-level understanding of optical flow. Optical flow is the motion of objects between consecutive frames of sequence, caused by the relative movement between the object or camera. The problem of optical flow may be expressed as:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "where between consecutive frames, we can express the image intensity $(I)$ as a function of space $(x,y)$ and time $(t)$. In other words, if we take the first image $I(x,y,t)$ and move its pixels by $(dx,dy)$ over $t$ time, we obtain the new image $I(x+dx,y+dy,t+dt)$.\n",
        "\n",
        "First, we assume that pixel intensities of an object are constant between consecutive frames and then Neighbouring pixels have similar motion.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "Second, we take the Taylor Series Approximation of the RHS and remove common terms.\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "Third, we divide by $dt$ to derive the optical flow equation:\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "where $u=dx/dt$ and $v=dy/dt$\n",
        "\n",
        "$dI/dx$, $dI/dy$, and $dI/dt$ are the image gradients along the horizontal axis, the vertical axis, and time. Hence, we conclude with the problem of optical flow, that is, solving $u(dx/dt)$ and $v(dy/dt)$ to determine movement over time. You may notice that we cannot directly solve the optical flow equation for $u$ and $v$ since there is only one equation for two unknown variables. We will implement some methods such as the Lucas-Kanade method to address this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePVM9K2WEtju"
      },
      "source": [
        "## Sparse vs Dense Optical Flow\n",
        "Sparse optical flow gives the flow vectors of some \"interesting features\" (say few pixels depicting the edges or corners of an object) within the frame while Dense optical flow, which gives the flow vectors of the entire frame (all pixels) - up to one flow vector per pixel. As you would've guessed, Dense optical flow has higher accuracy at the cost of being slow/computationally expensive.\n",
        "\n",
        "![image.png](https://nanonets.com/blog/content/images/2019/04/sparse-vs-dense.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXqVumzEtjx"
      },
      "source": [
        "## Implementing Sparse Optical Flow\n",
        "Sparse optical flow selects a sparse feature set of pixels (e.g. interesting features such as edges and corners) to track its velocity vectors (motion). The extracted features are passed in the optical flow function from frame to frame to ensure that the same points are being tracked. There are various implementations of sparse optical flow, including the Lucas–Kanade method, the Horn–Schunck method, the Buxton–Buxton method, and more. We will be using the Lucas-Kanade method with OpenCV, an open source library of computer vision algorithms, for implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzNjkjZ_Etjy"
      },
      "source": [
        "### Lucas-Kanade method\n",
        "We have seen an assumption before, that all the neighbouring pixels will have similar motion. Lucas-Kanade method takes a 3x3 pablem becomes solving 9 equations with two unknown variables which is over-determined. A better solution is obtained with least square fit tch around the point. So all the 9 points have the same motion. We can find $dI/dx$, $dI/dy$, and $dI/dt$ for these 9 points.\n",
        "\n",
        "So from the user point of view, the idea is simple, we give some points to track, we receive the optical flow vectors of those points. But again there are some problems. Until now, we were dealing with small motions, so it fails when there is a large motion. To deal with this we use pyramids. When we go up in the pyramid, small motions are removed and large motions become small motions. So by applying Lucas-Kanade there, we get optical flow along with the scale.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH_9asW6Etj0"
      },
      "source": [
        "### 1. Setting up your environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UadCDcm6Etj2",
        "outputId": "69074916-e4fc-4fbf-d077-f03c846e0739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEq_WnLiEtj8"
      },
      "source": [
        "### 2. Shi-Tomasi Corner Detector - selecting the pixels to track\n",
        "For the implementation of sparse optical flow, we only track the motion of a feature set of pixels. Features in images are points of interest which present rich image content information. For example, such features may be points in the image that are invariant to translation, scale, rotation, and intensity changes such as corners.\n",
        "\n",
        "The Shi-Tomasi Corner Detector is very similar to the popular Harris Corner Detector which can be implemented by the following three procedures:\n",
        "\n",
        "1. Determine windows (small image patches) with large gradients (variations in image intensity) when translated in both x and y directions.\n",
        "2. For each window, compute a score R\n",
        "3. Depending on the value of R, each window is classified as a flat, edge, or corner.\n",
        "\n",
        "If you would like to know more on a step-by-step mathematical explanation of the Harris Corner Detector, feel free to go through [these slides](https://courses.cs.washington.edu/courses/cse576/06sp/notes/HarrisDetector.pdf).\n",
        "\n",
        "Shi and Tomasi later made a small but effective modification to the Harris Corner Detector in their paper [Good Features to Track](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=323794).\n",
        "\n",
        "The modification is to the equation in which score R is calculated. In the Harris Corner Detector, the scoring function is given by:\n",
        "![image.png](attachment:image.png)\n",
        "Instead, Shi-Tomasi proposed the scoring function as:\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "which basically means if R is greater than a threshold, it is classified as a corner. The following compares the scoring functions of Harris (left) and Shi-Tomasi (right) in λ1−λ2 space.\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "For Shi-Tomasi, only when λ1 and λ2 are above a minimum threshold λmin is the window classified as a corner.\n",
        "The documentation of OpenCV’s implementation of Shi-Tomasi via `goodFeaturesToTrack()` may be found here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkLohHGUEtkC"
      },
      "source": [
        "### 3. Lucas-Kanade: Sparse Optical Flow\n",
        "Lucas and Kanade proposed an effective technique to estimate the motion of interesting features by comparing two consecutive frames in their paper An Iterative Image Registration Technique with an Application to Stereo Vision. The Lucas-Kanade method works under the following assumptions:\n",
        "\n",
        "1. Two consecutive frames are separated by a small time increment $(dt)$ such that objects are not displaced significantly (in other words, the method work best with slow-moving objects).\n",
        "2. A frame portrays a “natural” scene with textured objects exhibiting shades of gray that change smoothly.\n",
        "\n",
        "First, under these assumptions, we can take a small 3x3 window (neighborhood) around the features detected by Shi-Tomasi and assume that all nine points have the same motion.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "This may be represented as  \n",
        "\n",
        "![image-2.png](attachment:image-2.png)  \n",
        "\n",
        "where $q_1$, $q_2$, …, $q_n$ denote the pixels inside the window (e.g. n = 9 for a 3x3 window) and $I_x(q_i)$, $I_y(q_i)$, and $I_t(q_i)$ denote the partial derivatives of image $I$ with respect to position $(x,y)$ and time $t$, for pixel $q_i$ at the current time.\n",
        "\n",
        "This is just the Optical Flow Equation (that we described earlier) for each of the n pixels.\n",
        "\n",
        "The set of equations may be represented in the following matrix form where $Av=b$:\n",
        "\n",
        "![image-3.png](attachment:image-3.png)  \n",
        "\n",
        "Take note that previously (see \"What is optical flow?\" section), we faced the issue of having to solve for two unknown variables with one equation. We now face having to solve for two unknowns ($V_x$ and $V_y$) with nine equations, which is over-determined.\n",
        "\n",
        "Second, to address the over-determined issue, we apply least squares fitting to obtain the following two-equation-two-unknown problem:  \n",
        "\n",
        "![image-4.png](attachment:image-4.png)  \n",
        "\n",
        "where $V_x=u=dx/dt$ denotes the movement of $x$ over time and $V_y=v=dy/dt$ denotes the movement of $y$ over time. Solving for the two variables completes the optical flow problem.\n",
        "\n",
        "In a nutshell, we identify some interesting features to track and iteratively compute the optical flow vectors of these points. However, adopting the Lucas-Kanade method only works for small movements (from our initial assumption) and fails when there is large motion. Therefore, the OpenCV implementation of the Lucas-Kanade method adopts pyramids.\n",
        "\n",
        "![image-5.png](attachment:image-5.png)\n",
        "\n",
        "In a high-level view, small motions are neglected as we go up the pyramid and large motions are reduced to small motions - we compute optical flow along with scale. A comprehensive mathematical explanation of OpenCV’s implementation may be found in Bouguet’s notes and the documentation of OpenCV’s implementation of the Lucas-Kanade method via `calcOpticalFlowPyrLK()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "soldToWjEtkG"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kyGvvEonEtkJ"
      },
      "outputs": [],
      "source": [
        "# Parameters for Shi-Tomasi corner detection\n",
        "feature_params = dict(maxCorners=300, qualityLevel=0.2, minDistance=2, blockSize=7)\n",
        "\n",
        "# Parameters for Lucas-Kanade optical flow\n",
        "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "# The video feed is read in as a VideoCapture object\n",
        "cap = cv.VideoCapture(\"./videos/cars.mp4\")\n",
        "\n",
        "# Retrieve fps and frame size from input video\n",
        "fps = cap.get(cv.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Video writer to save output\n",
        "out_filename = \"./videos/output_sparse_optical_flow.mp4\"\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
        "output_writer = cv.VideoWriter(out_filename, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Variable for color to draw optical flow tracks\n",
        "color = (0, 255, 255)\n",
        "\n",
        "# Read the first frame from the video\n",
        "ret, first_frame = cap.read()\n",
        "if not ret:\n",
        "    print(\"Error: Unable to read the video file.\")\n",
        "    cap.release()\n",
        "    output_writer.release()\n",
        "    cv.destroyAllWindows()\n",
        "    exit()\n",
        "\n",
        "# Convert the first frame to grayscale\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "# Find the strongest corners in the first frame\n",
        "prev = cv.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
        "\n",
        "# Create an image filled with zero intensities with the same dimensions as the frame - for drawing purposes\n",
        "mask = np.zeros_like(first_frame)\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Read the current frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Exit loop if no more frames are available\n",
        "\n",
        "    # Convert the current frame to grayscale\n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate sparse optical flow using Lucas-Kanade method\n",
        "    prev = cv.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
        "    next, status, error = cv.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params)\n",
        "\n",
        "    # Select good feature points\n",
        "    good_old = prev[status == 1].astype(int)\n",
        "    good_new = next[status == 1].astype(int)\n",
        "\n",
        "    # Draw optical flow tracks\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "        mask = cv.line(mask, (a, b), (c, d), color, 2)\n",
        "        frame = cv.circle(frame, (a, b), 3, color, -1)\n",
        "\n",
        "    # Combine with the original frame\n",
        "    output = cv.add(frame, mask)\n",
        "\n",
        "    # Update the previous frame and points\n",
        "    prev_gray = gray.copy()\n",
        "    prev = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "    # Write the output frame to the video file\n",
        "    output_writer.write(output)\n",
        "\n",
        "    # Break the loop if 'q' is pressed\n",
        "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "output_writer.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL1sFY5fEtkM"
      },
      "source": [
        "## Implementing Dense Optical Flow\n",
        "We’ve previously computed the optical flow for a sparse feature set of pixels. Dense optical flow attempts to compute the optical flow vector for every pixel of each frame. While such computation may be slower, it gives a more accurate result and a denser result suitable for applications such as learning structure from motion and video segmentation. There are various implementations of dense optical flow. We will be using the Farneback method, one of the most popular implementations, with using OpenCV, an open source library of computer vision algorithms, for implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNAUCfnAEtkO"
      },
      "source": [
        "### Farneback Optical Flow\n",
        "Gunnar Farneback proposed an effective technique to estimate the motion of interesting features by comparing two consecutive frames in his paper [Two-Frame Motion Estimation Based on Polynomial Expansion](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf).\n",
        "\n",
        "First, the method approximates the windows (see Lucas Kanade section of sparse optical flow implementation for more details) of image frames by quadratic polynomials through polynomial expansion transform. Second, by observing how the polynomial transforms under translation (motion), a method to estimate displacement fields from polynomial expansion coefficients is defined. After a series of refinements, dense optical flow is computed. Farneback’s paper is fairly concise and straightforward to follow so I highly recommend going through the paper if you would like a greater understanding of its mathematical derivation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekQLXXFMEtkQ"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jrg-Uam3EtkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10088f75-dfa1-4922-a4b8-bdf8ae3ebc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No more frames to read or unable to read the frame. Exiting...\n"
          ]
        }
      ],
      "source": [
        "# The video feed is read in as a VideoCapture object\n",
        "cap = cv.VideoCapture(\"./videos/cars.mp4\")\n",
        "\n",
        "\n",
        "# Video writer to save output\n",
        "out_filename = \"./videos/output_dense_optical_flow.mp4\"\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
        "output_writer = cv.VideoWriter(out_filename, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "\n",
        "# ret = a boolean return value from getting the frame, first_frame = the first frame in the entire video sequence\n",
        "ret, first_frame = cap.read()\n",
        "\n",
        "if not ret:\n",
        "    print(\"Error: Unable to read the video file.\")\n",
        "    cap.release()\n",
        "    output_writer.release()\n",
        "    cv.destroyAllWindows()\n",
        "    exit()\n",
        "\n",
        "# Converts frame to grayscale because we only need the luminance channel for detecting edges - less computationally expensive\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "# Creates an image filled with zero intensities with the same dimensions as the frame\n",
        "mask = np.zeros_like(first_frame)\n",
        "\n",
        "# Sets image saturation to maximum\n",
        "mask[..., 1] = 255\n",
        "\n",
        "while cap.isOpened():\n",
        "    # ret = a boolean return value from getting the frame, frame = the current frame being projected in the video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"No more frames to read or unable to read the frame. Exiting...\")\n",
        "        break\n",
        "\n",
        "    # Converts each frame to grayscale - we previously only converted the first frame to grayscale\n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculates dense optical flow by Farneback method\n",
        "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "    # Computes the magnitude and angle of the 2D vectors\n",
        "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Sets image hue according to the optical flow direction\n",
        "    mask[..., 0] = angle * 180 / np.pi / 2\n",
        "\n",
        "    # Sets image value according to the optical flow magnitude (normalized)\n",
        "    mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
        "\n",
        "    # Converts HSV to RGB (BGR) color representation\n",
        "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
        "\n",
        "    # Write the output frame to the video file\n",
        "    output_writer.write(rgb)\n",
        "\n",
        "    # Updates previous frame\n",
        "    prev_gray = gray\n",
        "\n",
        "    # Frames are read by intervals of 1 millisecond. The program breaks out of the while loop when the user presses the 'q' key\n",
        "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# The following frees up resources and closes all windows\n",
        "cap.release()\n",
        "output_writer.release()\n",
        "cv.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}